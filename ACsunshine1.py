from __future__ import annotations

from typing import Tuple, Dict, Any

import matplotlib
import numpy as np
import pandas as pd
from scipy.optimize import minimize, differential_evolution
from torch import optim
import os

# ä¿®å¤ matplotlib ä¸­æ–‡ä¹±ç ï¼šä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“
matplotlib.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei", "SimSun", "KaiTi", "FangSong"]
matplotlib.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹æ¡†
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random

class ObjectiveFunctionError(Exception):
    """å½“ç›®æ ‡å‡½æ•°è®¡ç®—å¤±è´¥æ—¶æŠ›å‡ºçš„å¼‚å¸¸"""
    pass

class Actor(nn.Module):
    def __init__(self,state_dim=10,action_dim=5,hidden_dim=256):
        super(Actor,self).__init__()

        self.fc1 = nn.Linear(state_dim,hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim,hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)


        self.action_scale = 0.1  # æ›´å°æ­¥é•¿ï¼Œä¾¿äºåœ¨ 0.28 é™„è¿‘åšç²¾ç»†è°ƒæ•´

    def forward(self,state):
        x = self.fc1(state)
        x = self.ln1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = self.ln2(x)
        x = F.relu(x)
        raw_action =torch.tanh(self.fc3(x))
        scaled_action =raw_action * self.action_scale
        return scaled_action

class Env:
    def __init__(self,excel_path,params_bounds=None):
        super(Env,self).__init__()
        self.excel_path=excel_path
        self.V, self.I, self.I_min, self.I_max = self._load_data()

        # ä½¿ç”¨ float64ï¼šI0 ä¸‹ç•Œå¯èƒ½è®¾ä¸º 1e-60 ç­‰æå°å€¼ï¼Œfloat32 ä¼šä¸‹æº¢ä¸º 0 å¯¼è‡´ log(0)=-inf æŠ¥é”™
        self.param_bounds = np.array([
            [0.1, 10.0],   # I_ph
            [1e-60, 1e-6], # I0ï¼ˆfloat64 å¯è¡¨ç¤º 1e-60ï¼Œfloat32 æœ€å°çº¦ 1.2e-38ï¼‰
            [0.8, 1.5],    # n
            [0.001, 0.3],  # Rs
            [10.0, 300.0], # Rsh
        ], dtype=np.float64)

        self.errors={}

        self.default_params=np.array([
            5.05,  # I_ph: (0.1+10.0)/2 â‰ˆ 5.05
            5.05e-10,  # I0: å‡ ä½•ä¸­å€¼ âˆš(1e-15*1e-9)=1e-12ï¼Œå–5.05e-10
            1.1,  # n: (0.9+1.3)/2 = 1.1
            0.0775,  # Rs: (0.005+0.15)/2 = 0.0775
            77.5,  # Rsh: (5.0+150.0)/2 = 77.5
        ])
        self.state_dim=10
        self.action_dim=5
        # ç¯å¢ƒçŠ¶æ€
        self.current_params = None
        self.current_state = None
        self.step_count = 0
        self.best_error = float('inf')
        self.best_params = None
        self.prev_error = None
        self.prev_params = None

        # è®¡ç®—çƒ­ç”µå‹ (å‡è®¾æ¸©åº¦ä¸º25Â°C)
        self.Vt = 0.026  # çƒ­ç”µå‹ (V)

        # å¥–åŠ±å‡½æ•°æƒé‡ï¼šä¸»è¯¯å·®ä¸ºä¸»ï¼Œç‰©ç†é‡è¯¯å·®ä¸ºè¾…ï¼Œå¼•å¯¼ RL å…¼é¡¾æ•´ä½“ä¸å…³é”®ç‚¹
        self.reward_weights = {
            'main_error': 1.0,  # ä¸»è¯¯å·®ï¼ˆæ•´ä½“æ‹Ÿåˆï¼‰
            'boundary': 0.025,  # è¾¹ç•Œæƒ©ç½šï¼Œé¿å…å‚æ•°è´´è¾¹
            'mpp': 0.25,  # æœ€å¤§åŠŸç‡ç‚¹è¯¯å·®ï¼Œæ‹ç‚¹åŒºåŸŸå…³é”®
            'short_circuit': 0.15,  # çŸ­è·¯ç”µæµè¯¯å·®
            'open_voltage': 0.15,  # å¼€è·¯ç”µå‹è¯¯å·®
            'action_penalty': 0,  # åŠ¨ä½œæƒ©ç½šï¼ˆä¿æŒ 0ï¼Œç”± action_scale æ§åˆ¶ï¼‰
            'fill_factor': 0.1,  # å¡«å……å› å­è¯¯å·®ï¼Œåæ˜ æ•´ä½“æ›²çº¿å½¢çŠ¶
            'step_penalty': 0,  # æ­¥æ•°æƒ©ç½šï¼ˆä¿æŒ 0ï¼Œå…è®¸å……åˆ†æ¢ç´¢ï¼‰
        }

        # ç¨€ç–å¥–åŠ±é˜ˆå€¼å’Œå€¼ï¼ˆå¼ºåŒ– 0.1 ä»¥ä¸‹ç›®æ ‡ï¼‰
        self.sparse_thresholds = {
            'excellent': 0.01,  # å…¨å±€æœ€ä¼˜ï¼ˆæä½è¯¯å·®ï¼‰
            'good': 0.05,  # ä¸­æœŸè¿›å±•è‰¯å¥½
            'medium': 0.1,  # è¯¯å·® < 0.1 é‡Œç¨‹ç¢‘
            'near_target': 0.15,  # æ¥è¿‘ç›®æ ‡ï¼Œå¼•å¯¼å‘ 0.1 æ”¶æ•›
        }

        self.sparse_rewards = {
            'global_optimum': 10.0,  # å…¨å±€æœ€ä¼˜æ”¶æ•›å¥–åŠ±
            'key_progress': 2.0,  # ä¸­æœŸå…³é”®è¿›å±•å¥–åŠ±
            'target_01': 5.0,  # è¯¯å·® < 0.1 é¢å¤–å¥–åŠ±ï¼Œå¼ºåŒ–ç›®æ ‡
            'slow_converge': -5.0,  # æ…¢æ”¶æ•›æƒ©ç½š
            'severe_penalty': -2.0,  # ä¸¥é‡ç‰©ç†æ— æ•ˆæƒ©ç½š
        }

        # è®°å½•å·²è·å¾—çš„ç¨€ç–å¥–åŠ±
        self.achieved_milestones = set()
        self.no_improvement_steps = 0  # æ— æ”¹å–„æ­¥æ•°è®¡æ•°

        # é‡ç½®ç¯å¢ƒ
        self.reset()

    def _load_data(self):
        df = pd.read_excel(self.excel_path,header=None,usecols=[0,1],skiprows=1)
        V_orig =df.iloc[:,0].astype(float).values
        I_orig =df.iloc[:,1].astype(float).values

        vaild = np.isfinite(V_orig)&np.isfinite(I_orig)
        V=V_orig[vaild]
        I=I_orig[vaild]

        I_pos=I[I>0]
        if len(I_pos)==0:
            raise ValueError(f"æ•°æ®æ–‡ä»¶ '{self.excel_path}' æ— æ•ˆï¼šæœªæ‰¾åˆ°ä»»ä½•æ­£ç”µæµæ•°æ®ç‚¹ï¼ˆI > 0ï¼‰ã€‚\n")
        I_min =float(np.min(I_pos))
        I_max =float(np.max(I_pos))
        return V, I, I_min, I_max

    def _solar_cell_model(
            self,
            V: np.ndarray,
            params: np.ndarray,
    ) -> np.ndarray:
        # ğŸ”¥ å•äºŒæç®¡æ¨¡å‹ï¼š5ä¸ªå‚æ•° [I_ph, I0, n, Rs, Rsh]
        if len(params) != 5:
            raise ValueError(f"å•äºŒæç®¡æ¨¡å‹éœ€è¦5ä¸ªå‚æ•°ï¼Œå¾—åˆ°{len(params)}ä¸ª")
        I_ph, I0, n, Rs, Rsh = params

        Vt = 0.026
        clip_min, clip_max = -50.0, 150.0
        I_out = np.zeros_like(V, dtype=np.float64)
        prev_I = float(I_ph)  # ä¸Šä¸€ç”µå‹ç‚¹ç”µæµï¼Œç”¨ä½œé«˜å‹åŒºè¿­ä»£åˆå€¼ï¼Œåˆ©äºé™¡é™æ®µæ”¶æ•›

        # ä¿®å¤ï¼šclipèŒƒå›´åº”è¯¥åŸºäºç‰©ç†çº¦æŸï¼Œè€Œä¸æ˜¯æ•°æ®èŒƒå›´
        # ç”µæµåº”è¯¥åœ¨ [0, I_ph*1.5] èŒƒå›´ï¼Œè€Œä¸æ˜¯ [I_min*0.1, I_max*2]
        clip_min_current = 0.0  # ç”µæµä¸èƒ½ä¸ºè´Ÿ
        clip_max_current = I_ph * 1.5  # ç”µæµä¸èƒ½è¶…è¿‡I_phå¤ªå¤šï¼ˆè€ƒè™‘æµ‹é‡è¯¯å·®ï¼‰

        for i, v in enumerate(V):
            # å•äºŒæç®¡æ¨¡å‹ï¼šf(I) = I - (I_ph - I0*exp - shunt)
            def f(I_val: float) -> float:
                x = (v + I_val * Rs) / (n * Vt)
                x_clipped = np.clip(x, clip_min, clip_max)
                exp_term = np.exp(x_clipped) - 1.0
                shunt = (v + I_val * Rs) / Rsh
                return I_val - (I_ph - I0 * exp_term - shunt)

            def f_prime(I_val: float) -> float:
                x = (v + I_val * Rs) / (n * Vt)
                x_clipped = np.clip(x, clip_min, clip_max)
                exp_term = np.exp(x_clipped)
                return 1.0 + (I0 * Rs / (n * Vt)) * exp_term + Rs / Rsh

            init_I = prev_I if i > 0 else float(I_ph)
            for _ in range(3):  # é¢„çƒ­è¿­ä»£ï¼Œä¿è¯åˆå€¼è¶³å¤Ÿå¥½
                if not np.isfinite(init_I):
                    init_I = I_ph  # å¦‚æœå¼‚å¸¸ï¼Œé‡ç½®ä¸ºI_ph
                    break
                if init_I <= 0:
                    init_I = I_ph * 0.95  # å¦‚æœä¸ºè´Ÿæˆ–0ï¼Œè®¾ç½®ä¸ºæ¥è¿‘I_phçš„å€¼
                    break
                fp = f_prime(init_I)
                if abs(fp) < 1e-12:
                    break
                init_I = init_I - f(init_I) / fp
                # ä¿®å¤ï¼šä½¿ç”¨ç‰©ç†çº¦æŸï¼Œç¡®ä¿ç”µæµåœ¨åˆç†èŒƒå›´
                init_I = float(np.clip(init_I, 0.0, clip_max_current))

            # ä¿®å¤ï¼šç¡®ä¿åˆå§‹å€¼åˆç†
            if init_I <= 0 or not np.isfinite(init_I):
                init_I = I_ph * 0.95  # å¦‚æœåˆå§‹å€¼å¼‚å¸¸ï¼Œä½¿ç”¨æ¥è¿‘I_phçš„å€¼

            I_i = init_I
            for iter_count in range(50):  # æ¢å¤è¶³å¤Ÿè¿­ä»£ï¼Œä¿è¯æ‹Ÿåˆé˜¶æ®µæ¨¡å‹ç²¾åº¦
                # ä½¿ç”¨ç‰›é¡¿æ³•è€Œéå›ºå®šç‚¹è¿­ä»£ï¼Œæé«˜é«˜ç”µå‹åŒºæ”¶æ•›æ€§ï¼Œç¡®ä¿é™¡é™æ®µèƒ½æ­£ç¡®è®¡ç®—
                f_val = f(I_i)
                fp_val = f_prime(I_i)
                if abs(fp_val) > 1e-12:
                    I_new = I_i - f_val / fp_val
                else:
                    # å…œåº•ï¼šå¦‚æœå¯¼æ•°å¤ªå°ï¼Œä½¿ç”¨ç®€åŒ–å…¬å¼
                    x = (v + I_i * Rs) / (n * Vt)
                    x_clipped = np.clip(x, clip_min, clip_max)
                    exp_term = np.exp(x_clipped) - 1.0
                    shunt = (v + I_i * Rs) / Rsh
                    I_new = I_ph - I0 * exp_term - shunt

                # ç‰©ç†çº¦æŸï¼šç”µæµåœ¨ [0, I_ph*1.5]ï¼Œä¸å¼ºåˆ¶ä½ç”µå‹ä¸‹é™ä»¥å…æ‰­æ›²æ‹Ÿåˆ
                I_new = float(np.clip(I_new, 0.0, clip_max_current))

                if abs(I_new - I_i) < 1e-8:
                    I_i = I_new
                    break
                if iter_count >= 25 and abs(I_new - I_i) < 1e-5:
                    I_i = I_new
                    break
                I_i = I_new

            # ä»…å½“è¿­ä»£ç»“æœå¼‚å¸¸æ—¶ä½¿ç”¨å…œåº•ï¼Œé¿å…è¿‡åº¦çº¦æŸæ‰­æ›²æ‹Ÿåˆ
            if I_i <= 0 or not np.isfinite(I_i):
                I_i = max(I_ph - v / Rsh, I_ph)

            I_out[i] = float(I_i)
            prev_I = float(I_out[i])

        return I_out

    def _objective_function(self,params: np.ndarray, V: np.ndarray, I_meas: np.ndarray) -> float:
        """
        ä½¿ç”¨ I_max å½’ä¸€åŒ–çš„æ®‹å·®ï¼Œé¿å…ä½ç”µæµç‚¹ï¼ˆå¼€è·¯é™„è¿‘ï¼‰ç›¸å¯¹è¯¯å·®çˆ†ç‚¸ã€‚
        ä¸ traditional_fit_test ä¸€è‡´ï¼Œåˆ©äºä¼ ç»Ÿä¼˜åŒ–æ”¶æ•›åˆ° ~0.3 é‡çº§ã€‚
        """
        # è®¡ç®—æ¨¡æ‹Ÿç”µæµ
        I_sim = self._solar_cell_model(V, params)

        # é€‰æ‹©æœ‰æ•ˆç‚¹ï¼ˆæµ‹é‡ç”µæµ>0ï¼‰
        valid = (I_meas > 1e-10) & np.isfinite(I_meas) & np.isfinite(I_sim) & (I_sim > 0)
        if not np.any(valid):
            return 1e10

        I_m = I_meas[valid]
        I_s = I_sim[valid]
        I_max_ref = float(np.max(I_m))  # ç”¨æµ‹é‡ç”µæµæœ€å¤§å€¼å½’ä¸€åŒ–ï¼Œé¿å… I_m å¾ˆå°æ—¶çˆ†ç‚¸

        # å½’ä¸€åŒ–æ®‹å·®ï¼š(I_m - I_s) / I_max_refï¼Œå°ºåº¦ç¨³å®š
        residuals = (I_m - I_s) / (I_max_ref + 1e-12)
        loss = np.sqrt(np.mean(residuals ** 2))

        if not np.isfinite(loss):
            raise ObjectiveFunctionError("ç›®æ ‡å‡½æ•°è®¡ç®—ç»“æœä¸ºæ— ç©·å¤§æˆ–NaN")

        return float(loss)

    def _normalize_params(self, params: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–å‚æ•°åˆ°[0, 1]èŒƒå›´"""
        norm_params = np.zeros_like(params)

        for i in range(len(params)):
            min_val, max_val = self.param_bounds[i]
            norm_params[i] = (params[i] - min_val) / (max_val - min_val)
            norm_params[i] = np.clip(norm_params[i], 0, 1)

        return norm_params

    def _denormalize_params(self, norm_params: np.ndarray) -> np.ndarray:
        """åå½’ä¸€åŒ–å‚æ•°ã€‚ä½¿ç”¨ float64 ä»¥ä¿è¯ I0 ç­‰æå°å€¼ï¼ˆå¦‚ 1e-60ï¼‰ä¸ä¼šåœ¨ float32 ä¸­ä¸‹æº¢ä¸º 0ã€‚"""
        params = np.zeros(len(norm_params), dtype=np.float64)
        for i in range(len(norm_params)):
            min_val, max_val = self.param_bounds[i]
            params[i] = min_val + float(norm_params[i]) * (max_val - min_val)
        return params

    def _calculate_errors(self,params:np.ndarray)->Dict[str,float]:
        main_error =self._objective_function(params,self.V,self.I)
        I_calc = self._solar_cell_model(self.V, params)
        # MPPè¯¯å·®,æœ€å¤§åŠŸç‡è¯¯å·®ï¼Œé€šå¸¸åœ¨æ‹ç‚¹
        if len(self.V) > 0 and len(I_calc) > 0:
            P_meas = self.V * self.I
            P_calc = self.V * I_calc
            mpp_meas = np.max(P_meas) if len(P_meas) > 0 else 0
            mpp_calc = np.max(P_calc) if len(P_calc) > 0 else 0
            mpp_error = abs(mpp_meas - mpp_calc) / mpp_meas if mpp_meas > 0 else abs(mpp_meas - mpp_calc)
        else:
            mpp_error = 0
        # çŸ­è·¯ç”µæµè¯¯å·®ï¼ˆç®€åŒ–çš„è®¡ç®—ï¼‰
        if len(self.V) > 0:
            zero_voltage_idx = np.argmin(np.abs(self.V))
            short_circuit_abs_error = abs(I_calc[zero_voltage_idx] - self.I[zero_voltage_idx])
            short_circuit_error=short_circuit_abs_error/self.I_max
            short_circuit_error=np.clip(short_circuit_error,0,1)
        else:
            short_circuit_error = 0

        #å¼€è·¯ç”µå‹è¯¯å·®ï¼ˆå½’ä¸€åŒ–å¤„ç†ï¼‰
        min_I_idx=np.argmin(np.abs(self.I))
        V_at_min_I=self.V[min_I_idx]
        I_meas_min=self.I[min_I_idx]
        I_calc_at_Vmin=self._solar_cell_model(np.array([V_at_min_I]),params)[0]
        open_voltage_error=abs(I_calc_at_Vmin-I_meas_min)
        ov_error_norm =open_voltage_error/self.I_max
        ov_error_norm=np.clip(ov_error_norm,0,1)

        # æ–°å¢ï¼šå¡«å……å› å­è¯¯å·®
        # æµ‹é‡æ•°æ®çš„å¡«å……å› å­
        I_sc_meas = self.I[np.argmin(np.abs(self.V))]
        V_oc_meas = self.V[np.argmin(np.abs(self.I))]
        P_max_meas = np.max(self.V * self.I) if len(self.V) > 0 else 0
        FF_meas = P_max_meas / (V_oc_meas * I_sc_meas) if V_oc_meas > 0 and I_sc_meas > 0 else 0

        # æ¨¡æ‹Ÿæ•°æ®çš„å¡«å……å› å­
        I_sc_sim = I_calc[np.argmin(np.abs(self.V))]
        V_oc_sim = self.V[np.argmin(np.abs(I_calc))]
        P_max_sim = np.max(self.V * I_calc) if len(self.V) > 0 else 0
        FF_sim = P_max_sim / (V_oc_sim * I_sc_sim) if V_oc_sim > 0 and I_sc_sim > 0 else 0

        fill_factor_error = abs(FF_meas - FF_sim) / FF_meas if FF_meas > 0 else abs(FF_meas - FF_sim)
        fill_factor_error = np.clip(fill_factor_error, 0, 1)  # å½’ä¸€åŒ–

        return {
            'main_error':main_error,
            'mpp_error':mpp_error,
            'short_circuit_error':short_circuit_error,
            'ov_error_norm': ov_error_norm,
            'fill_factor_error': fill_factor_error,
        }

    def _check_physical_validity(self, params: np.ndarray) -> bool:
        """æ£€æŸ¥ç‰©ç†æœ‰æ•ˆæ€§ï¼šå‚æ•°åœ¨è¾¹ç•Œå†…ã€æ¨¡æ‹Ÿç”µæµæ­£å¸¸ã€è¯¯å·®æœªè¿‡å¤§"""
        # å‚æ•°è¾¹ç•Œæ£€æŸ¥ï¼ˆå…è®¸è¶…å‡º5%ï¼‰
        for i, (low, high) in enumerate(self.param_bounds):
            margin = (high - low) * 0.05
            if params[i] < low - margin or params[i] > high + margin:
                return False
        # æ¨¡æ‹Ÿç”µæµæ£€æŸ¥
        I_sim = self._solar_cell_model(self.V, params)
        if not np.all(np.isfinite(I_sim)):
            return False
        # ä¸»è¯¯å·®æ£€æŸ¥ï¼ˆè¶…è¿‡1.0è§†ä¸ºä¸¥é‡æ— æ•ˆï¼‰
        main_error = self._objective_function(params, self.V, self.I)
        if main_error > 1.0:
            return False
        return True

    def _calculate_boundary_penalty(self, params: np.ndarray) -> float:
        """è¾¹ç•Œæƒ©ç½šï¼šå‚æ•°è¶Šé è¿‘è¾¹ç•Œæƒ©ç½šè¶Šå¤§ï¼ŒæŒ‡æ•°å½¢å¼"""
        penalty = 0.0
        for i, (low, high) in enumerate(self.param_bounds):
            # è®¡ç®—åˆ°è¾¹ç•Œçš„è·ç¦»ï¼ˆå–æœ€è¿‘è¾¹ç•Œçš„è·ç¦»ï¼‰
            dist_to_low = params[i] - low
            dist_to_high = high - params[i]
            if dist_to_low < 0 or dist_to_high < 0:
                # è¶…å‡ºè¾¹ç•Œï¼Œç›´æ¥ç»™å¤§æƒ©ç½šï¼ˆä½†ä¸¥é‡æƒ©ç½šä¼šåœ¨ç‰©ç†æœ‰æ•ˆæ€§ä¸­å¤„ç†ï¼‰
                penalty += 10.0
            else:
                # å½’ä¸€åŒ–è·ç¦»
                range_len = high - low
                norm_dist = min(dist_to_low, dist_to_high) / (range_len * 0.1)  # 0.1å€èŒƒå›´ä½œä¸ºå°ºåº¦
                penalty += np.exp(-norm_dist)
        return penalty

    def _calculate_reward(self, action: np.ndarray, current_error: float, done: bool) -> float:
        """
        ç»¼åˆå¥–åŠ±å‡½æ•°ï¼ˆé‡æ„ï¼šåªæœ‰é™è¯¯å·®æ‰æœ‰å¥–ï¼Œä¸é™å°±æƒ©ç½šï¼Œé¿å…ä¸åŠ¨å°±æœ‰å¥–å¯¼è‡´å±€éƒ¨æœ€ä¼˜ï¼‰
        - æ ¸å¿ƒï¼šå¥–åŠ±ä¸â€œæœ¬æ­¥è¯¯å·®ç›¸å¯¹ä¸Šä¸€æ­¥çš„ä¸‹é™é‡â€æŒ‚é’©ï¼Œä¸é™åˆ™ç»™æƒ©ç½šã€‚
        """
        errors = self._calculate_errors(self.current_params)
        reward = 0.0

        # 1. æ ¸å¿ƒï¼šä»…å‡­â€œè¯¯å·®æ˜¯å¦ä¸‹é™â€ç»™å¥–/ç½šï¼ˆä¸å†æœ‰ base_rewardï¼Œä¸åŠ¨æ— å¥–ï¼‰
        if self.prev_error is not None:
            delta = self.prev_error - current_error  # æ­£å€¼è¡¨ç¤ºè¯¯å·®ä¸‹é™
            if delta > 0:
                # é™è¯¯å·®æ‰æœ‰å¥–ï¼šå¥–åŠ±ä¸ä¸‹é™é‡æˆæ­£æ¯”ï¼Œä½è¯¯å·®åŒºé—´æ”¾å¤§ç³»æ•°
                scale = 80.0 if current_error <= 0.35 else 20.0
                reward += scale * delta
            else:
                # ä¸é™å°±æƒ©ç½šï¼›æƒ©ç½šä¸å®œè¿‡å¤§ï¼Œå¦åˆ™ Q å…¨è´Ÿã€ç­–ç•¥æ¢¯åº¦éš¾ä»¥å­¦ä¹ ï¼ˆæ˜“å¡åœ¨ 0.34ï¼‰
                reward -= 0.35
        # ç¬¬ä¸€æ­¥ prev_error ä¸º Noneï¼Œä¸å¥–ä¸ç½šæ”¹è¿›é¡¹ï¼Œä»…ä¿ç•™è¾¹ç•Œ/ç‰©ç†æƒ©ç½š

        # 2. è¾¹ç•Œæƒ©ç½šï¼ˆæ¬¡è¦ï¼Œé¿å…å‚æ•°è´´è¾¹ï¼‰
        boundary_pen = self._calculate_boundary_penalty(self.current_params)
        reward -= self.reward_weights['boundary'] * boundary_pen

        # 3. å„ç‰©ç†é‡è¯¯å·®æƒ©ç½šï¼ˆæ¬¡è¦ï¼Œå¼•å¯¼æ›²çº¿å½¢çŠ¶ï¼‰
        reward -= self.reward_weights['mpp'] * (errors['mpp_error']**2)
        reward -= self.reward_weights['short_circuit'] * (errors['short_circuit_error']**2)
        reward -= self.reward_weights['open_voltage'] * (errors['ov_error_norm']**2)
        reward -= self.reward_weights['fill_factor'] * (errors['fill_factor_error']**2)

        # 4. ç¨€ç–å¥–åŠ±ï¼šä»…å½“â€œé¦–æ¬¡çªç ´â€æŸè¯¯å·®é˜ˆå€¼æ—¶ç»™ä¸€æ¬¡æ€§å¥–åŠ±ï¼ˆå±äºé™è¯¯å·®çš„é‡Œç¨‹ç¢‘ï¼‰
        milestone_map = {
            'init_break': 0.30,
            'break_028': 0.28,
            'break_025': 0.25,
            'break_020': 0.20
        }
        for milestone, thr in milestone_map.items():
            if current_error < thr and milestone not in self.achieved_milestones:
                reward += 10.0 if milestone == 'break_028' else 5.0
                self.achieved_milestones.add(milestone)

        if current_error < self.sparse_thresholds['excellent'] and 'excellent' not in self.achieved_milestones:
            reward += self.sparse_rewards['global_optimum'] * 1.0
            self.achieved_milestones.add('excellent')

        # 5. è¿ç»­å¤šæ­¥æ— æ”¹å–„æ—¶çš„é¢å¤–æƒ©ç½š
        if self.no_improvement_steps > 100:
            reward -= 2.0
            self.no_improvement_steps = 0

        reward = np.clip(reward, -5.0, 15.0)  # é™åˆ¶å¥–åŠ±èŒƒå›´ï¼Œç¨³å®šQå€¼ä¼°è®¡

        return float(reward)


    def reset(self, perturb: bool = False, perturb_scale: float = 0.02, initial_params: np.ndarray | None = None) -> np.ndarray:
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€ã€‚
        initial_params: è‹¥ç»™å‡ºï¼Œåˆ™ä»è¯¥å‚æ•°èµ·æ­¥ï¼ˆç”¨äºä»å†å²æœ€ä½³çƒ­å¯åŠ¨ï¼‰ï¼›å¦åˆ™ä» default_params èµ·æ­¥ã€‚
        perturb=True æ—¶åœ¨èµ·æ­¥ç‚¹ä¸ŠåŠ å°æ‰°åŠ¨ã€‚
        """
        if initial_params is not None:
            self.current_params = np.asarray(initial_params, dtype=np.float64).copy()
        else:
            self.current_params = self.default_params.copy()
        if perturb:
            norm = self._normalize_params(self.current_params)
            norm = norm + np.random.uniform(-perturb_scale, perturb_scale, 5)
            norm = np.clip(norm, 0, 1)
            self.current_params = self._denormalize_params(norm)
        self.step_count = 0
        self.prev_error = None
        self.prev_params = None
        self.achieved_milestones = set()
        self.no_improvement_steps = 0

        # è®¡ç®—åˆå§‹è¯¯å·®å¹¶æ„å»ºçŠ¶æ€ï¼›ç”¨åˆå§‹è§£ä½œä¸ºå½“å‰æœ€ä½³ï¼Œé¿å…ç¬¬ä¸€æ­¥æ¢ç´¢å°±è¦†ç›–æ‰ä½è¯¯å·®èµ·ç‚¹
        self.errors = self._calculate_errors(self.current_params)
        self.best_error = float(self.errors['main_error'])
        self.best_params = self.current_params.copy()
        self.current_state = self._build_state(self.current_params, self.errors)
        return self.current_state

    def _build_state(self, params: np.ndarray, errors: Dict[str, float]) -> np.ndarray:
        """
        æ„å»º10ç»´çŠ¶æ€å‘é‡ï¼š
        - 5ä¸ªå½’ä¸€åŒ–å‚æ•°
        - 5ä¸ªè¯¯å·®æŒ‡æ ‡ï¼ˆä¸»è¯¯å·®å½’ä¸€åŒ–ã€æ­¥æ•°å½’ä¸€åŒ–ã€MPPè¯¯å·®å½’ä¸€åŒ–ã€çŸ­è·¯è¯¯å·®ã€å¼€è·¯è¯¯å·®ï¼‰
        """
        norm_params = self._normalize_params(params)

        err = errors['main_error']
        main_error_norm = err/0.5
        main_error_norm=np.clip(main_error_norm,0,1)
        step_norm = min(self.step_count / 1200.0, 1.0)  # ä¸ MAX_STEPS ä¸€è‡´
        mpp_error_norm = errors['mpp_error']  # å·²ç»æ˜¯0~1ä¹‹é—´ï¼ˆç›¸å¯¹è¯¯å·®ï¼‰
        sc_error_norm = errors['short_circuit_error']  # å·²å½’ä¸€åŒ–
        ov_error_norm = errors['ov_error_norm']  # å·²å½’ä¸€åŒ–

        state = np.array([
            norm_params[0], norm_params[1], norm_params[2], norm_params[3], norm_params[4],
            main_error_norm,
            step_norm,
            mpp_error_norm,
            sc_error_norm,
            ov_error_norm
        ], dtype=np.float32)
        return state

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œã€‚
        action: 5ç»´æ•°ç»„ï¼Œå»ºè®®èŒƒå›´ [-0.2, 0.2]ï¼ˆå½’ä¸€åŒ–ç©ºé—´å†…çš„è°ƒæ•´é‡ï¼‰
        è¿”å›: (next_state, reward, done, info)
        """
        self.step_count += 1


        # 2. å‚æ•°æ›´æ–°ï¼ˆåœ¨å½’ä¸€åŒ–ç©ºé—´å†…ï¼‰
        current_norm = self._normalize_params(self.current_params)
        new_norm = current_norm + action
        new_norm = np.clip(new_norm, 0, 1)
        new_params = self._denormalize_params(new_norm)

        # 3. è®¡ç®—æ–°å‚æ•°ä¸‹çš„è¯¯å·®
        current_error = self._objective_function(new_params, self.V, self.I)  # æ³¨æ„ä¼ å…¥V,I
        errors = self._calculate_errors(new_params)

        # 4. æ›´æ–°æœ€ä½³è®°å½•å’Œæ— æ”¹å–„æ­¥æ•°
        if current_error < self.best_error:
            self.best_error = current_error
            self.best_params = new_params.copy()
            self.no_improvement_steps = 0
        else:
            self.no_improvement_steps += 1

        # 5. æ›´æ–°çŠ¶æ€
        self.current_params = new_params
        self.current_state = self._build_state(new_params, errors)

        # 6. è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action, current_error, done=False)  # doneæš‚æ—¶Falseï¼Œåé¢å†åˆ¤æ–­

        # 7. åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢
        done = self._check_done()

        # 8. ä¿å­˜å†å²ï¼ˆä¾›å¹³æ»‘æƒ©ç½šç­‰ä½¿ç”¨ï¼‰
        self.prev_error = current_error
        self.prev_params = new_params.copy()

        # 9. æ„å»ºinfoå­—å…¸
        info = {
            'step': self.step_count,
            'objective_error': current_error,
            'best_objective_error': self.best_error,
            'params': new_params.copy(),
            'errors': errors.copy(),
            'no_improvement_steps': self.no_improvement_steps,
            'achieved_milestones': list(self.achieved_milestones),
        }

        return self.current_state, reward, done, info

    def _check_done(self) -> bool:
        """æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶"""
        if self.best_error < 1e-4:
            return True
        if self.step_count >= 1200:
            return True
        if self.no_improvement_steps >= 200:
            return True
        return False

class Critic(nn.Module):
    def __init__(self, state_dim=10, action_dim=5, hidden_dim=512):
        super(Critic, self).__init__()
        # å°†çŠ¶æ€å’ŒåŠ¨ä½œæ‹¼æ¥åè¾“å…¥
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.ln3 = nn.LayerNorm(hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, 1)  # è¾“å‡º Q(s,a)

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)  # æ‹¼æ¥
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = self.ln2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = self.ln3(x)
        x = F.relu(x)
        q = self.fc4(x)
        return q



class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done

    def __len__(self):
        return len(self.buffer)

class TD3Agent:
    """
    TD3 (Twin Delayed DDPG) ç®—æ³•ï¼Œè§£å†³ Q å€¼è¿‡ä¼°è®¡é—®é¢˜ï¼š
    1. Clipped Double Q-learning: åŒ Critic å–æœ€å°å€¼ä½œä¸ºç›®æ ‡
    2. Target Policy Smoothing: å¯¹ç›®æ ‡åŠ¨ä½œåŠ å™ªå£°ï¼Œå¹³æ»‘ç­–ç•¥
    3. Delayed Policy Updates: å»¶è¿Ÿ Actor æ›´æ–°
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256,
                 lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.005,
                 buffer_capacity=100000, batch_size=64,
                 policy_noise=0.2, noise_clip=0.5, policy_delay=2):
        self.update_count = 0
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.action_dim = action_dim
        self.policy_noise = policy_noise  # ç›®æ ‡ç­–ç•¥å¹³æ»‘å™ªå£°æ ‡å‡†å·®
        self.noise_clip = noise_clip      # å™ªå£°è£å‰ªèŒƒå›´
        self.policy_delay = policy_delay  # Actor å»¶è¿Ÿæ›´æ–°é—´éš”

        # ä¸»ç½‘ç»œ
        self.actor = Actor(state_dim, action_dim, hidden_dim)
        self.actor.id = id(self.actor)

        # TD3: åŒ Critic ç½‘ç»œï¼Œç¼“è§£ Q å€¼è¿‡ä¼°è®¡
        self.critic1 = Critic(state_dim, action_dim, hidden_dim)
        self.critic2 = Critic(state_dim, action_dim, hidden_dim)

        # ç›®æ ‡ç½‘ç»œ
        self.actor_target = Actor(state_dim, action_dim, hidden_dim)
        self.critic1_target = Critic(state_dim, action_dim, hidden_dim)
        self.critic2_target = Critic(state_dim, action_dim, hidden_dim)

        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())

        # ä¼˜åŒ–å™¨
        self.optim_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.optim_critic = optim.Adam(
            list(self.critic1.parameters()) + list(self.critic2.parameters()),
            lr=lr_critic
        )

        self.replay_buffer = ReplayBuffer(buffer_capacity)
        self.noise = GaussianNoise(action_dim)

    def select_action(self, state, add_noise=True):
        self.actor.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_tensor = self.actor(state_tensor)
            action = action_tensor.cpu().numpy()[0]
        self.actor.train()
        if add_noise:
            noise = self.noise.sample()
            action += noise
        return np.clip(action, -self.actor.action_scale, self.actor.action_scale)

    def update(self):
        if len(self.replay_buffer) < self.batch_size:
            return
        self.update_count += 1

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # -------------------- TD3: æ›´æ–°åŒ Critic --------------------
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘ï¼šå¯¹ç›®æ ‡åŠ¨ä½œåŠ å™ªå£°ï¼Œç¼“è§£ Q è¿‡ä¼°è®¡
            next_actions = self.actor_target(next_states)
            noise = (torch.randn_like(next_actions) * self.policy_noise).clamp(
                -self.noise_clip, self.noise_clip
            )
            next_actions_smooth = (next_actions + noise).clamp(
                -self.actor.action_scale, self.actor.action_scale
            )

            # Clipped Double Q: å–ä¸¤ä¸ª Q çš„æœ€å°å€¼ä½œä¸ºç›®æ ‡ï¼Œå‡å°‘è¿‡ä¼°è®¡
            target_q1 = self.critic1_target(next_states, next_actions_smooth)
            target_q2 = self.critic2_target(next_states, next_actions_smooth)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + self.gamma * (1 - dones) * target_q

        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        critic1_loss = nn.SmoothL1Loss()(current_q1, target_q)
        critic2_loss = nn.SmoothL1Loss()(current_q2, target_q)
        critic_loss = critic1_loss + critic2_loss

        self.optim_critic.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), max_norm=1.0)
        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), max_norm=1.0)
        self.optim_critic.step()

        # -------------------- TD3: å»¶è¿Ÿç­–ç•¥æ›´æ–° --------------------
        if self.update_count % self.policy_delay == 0:
            # Actor ä»…ä½¿ç”¨ Q1 çš„æ¢¯åº¦ï¼ˆå‡å°‘æ–¹å·®ï¼‰
            actor_loss = -self.critic1(states, self.actor(states)).mean()
            self.optim_actor.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
            self.optim_actor.step()

            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)
            for target_param, param in zip(self.critic1_target.parameters(), self.critic1.parameters()):
                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)
            for target_param, param in zip(self.critic2_target.parameters(), self.critic2.parameters()):
                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)
        else:
            with torch.no_grad():
                actor_loss = -self.critic1(states, self.actor(states)).mean()

        if self.update_count % 100 == 0:
            q_mean = (current_q1.mean().item() + current_q2.mean().item()) / 2
            print(f"Update {self.update_count}: Critic loss = {critic_loss.item():.4f}, "
                  f"Q mean = {q_mean:.4f}, Actor loss = {actor_loss.item():.4f}")

class GaussianNoise:
    """
    ç”¨äº DDPG æ¢ç´¢çš„é«˜æ–¯å™ªå£°ç”Ÿæˆå™¨ã€‚
    å‚æ•°ï¼š
        action_dim (int): åŠ¨ä½œç©ºé—´çš„ç»´åº¦ã€‚
        std (float): åˆå§‹æ ‡å‡†å·®ï¼Œæ§åˆ¶å™ªå£°çš„å¹…åº¦ï¼Œé»˜è®¤ä¸º 0.1ã€‚
        std_decay (float): æ¯æ¬¡è°ƒç”¨ sample() åæ ‡å‡†å·®çš„è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ä¸º 0.999ã€‚
        std_min (float): æ ‡å‡†å·®çš„æœ€å°å€¼ï¼Œé˜²æ­¢è¡°å‡åˆ°é›¶ï¼Œé»˜è®¤ä¸º 0.01ã€‚
    """
    def __init__(self, action_dim, std=0.1, std_decay=0.999, std_min=0.01):
        self.action_dim = action_dim
        self.std = std
        self.std_decay = std_decay
        self.std_min = std_min
        self.std_original = std  # ä¿å­˜åˆå§‹å€¼ï¼Œä¾¿äºé‡ç½®

    def sample(self):
        """
        ç”Ÿæˆä¸€ä¸ªå™ªå£°å‘é‡ï¼Œå½¢çŠ¶ä¸º (action_dim,)ã€‚
        æ¯æ¬¡è°ƒç”¨åï¼Œæ ‡å‡†å·®æŒ‰è¡°å‡ç³»æ•°å‡å°ï¼ˆä½†ä¸ä¼šä½äºæœ€å°å€¼ï¼‰ã€‚
        """
        noise = np.random.normal(0, self.std, size=self.action_dim)
        # æ›´æ–°æ ‡å‡†å·®
        self.std = max(self.std * self.std_decay, self.std_min)
        return noise

    def reset(self):
        """
        å°†æ ‡å‡†å·®é‡ç½®ä¸ºåˆå§‹å€¼ã€‚å¯åœ¨æ¯ä¸ª episode å¼€å§‹æ—¶è°ƒç”¨ï¼Œ
        ä½¿å™ªå£°åœ¨æ¯ä¸ª episode é‡æ–°å¼€å§‹è¡°å‡ã€‚
        """
        self.std = self.std_original


def main():
    # ========== è¶…å‚æ•°è®¾ç½® ==========
    EXCEL_PATH = r"C:\Users\18372\PycharmProjects\pythonProject1\2 (1).xls"  # è¯·æ›¿æ¢ä¸ºå®é™…æ•°æ®æ–‡ä»¶è·¯å¾„
    STATE_DIM = 10
    ACTION_DIM = 5
    HIDDEN_DIM = 256  # æé«˜å®¹é‡ï¼Œé¿å…è¡¨è¾¾èƒ½åŠ›ä¸è¶³å¯¼è‡´è¯¯å·®å¡åœ¨ 0.34 é™„è¿‘
    LR_ACTOR = 3e-4
    LR_CRITIC = 1e-4  # è¿›ä¸€æ­¥é™ä½ä»¥ç¨³å®š Criticï¼Œå‡å°‘ loss å°–å³°
    GAMMA = 0.95
    TAU = 0.001  # ç›®æ ‡ç½‘ç»œæ›´æ–°æ›´æ…¢ï¼ŒQ ä¼°è®¡æ›´ç¨³å®š
    BUFFER_CAPACITY = 100000
    BATCH_SIZE = 128  # æ›´å¤§ batch ä½¿æ¢¯åº¦æ›´å¹³æ»‘
    NUM_EPISODES = 500
    MAX_STEPS = 1200  # æ›´å¤šæ­¥æ•°ï¼Œç»™ RL è¶³å¤Ÿæ—¶é—´ä» 0.28 ç²¾ç»†è°ƒæ•´
    NOISE_STD = 0.06   # é€‚å½“å¢å¤§æ¢ç´¢å™ªå£°ï¼Œä¾¿äºè·³å‡ºå±€éƒ¨æœ€ä¼˜
    NOISE_DECAY = 0.998 # è¡°å‡ç¨æ…¢ï¼Œå‰æœŸæ¢ç´¢æ›´å……åˆ†
    NOISE_MIN = 0.008  # æé«˜æœ€å°å™ªå£°ï¼ŒåæœŸä»æœ‰ä¸€å®šæ¢ç´¢
    TARGET_ERROR = 1e-4  # è¾¾åˆ°æ­¤è¯¯å·®æå‰åœæ­¢
    SAVE_DIR = "./models"
    os.makedirs(SAVE_DIR, exist_ok=True)

    # ========== åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“ ==========
    env = Env(excel_path=EXCEL_PATH)

    # ---------- ä¼ ç»Ÿä¼˜åŒ–ï¼šç”¨ scipy æ‹Ÿåˆå¾—åˆ°æ›´å¥½çš„é»˜è®¤å‚æ•° ----------
    # I0 è·¨å¤šä¸ªæ•°é‡çº§ï¼Œåœ¨ log ç©ºé—´ä¼˜åŒ–æ›´æ˜“æ”¶æ•›ï¼›è¾¹ç•Œä¸ Env ä¸€è‡´ï¼Œä» param_bounds è¯»å–
    I0_LOW = float(env.param_bounds[1, 0])
    I0_HIGH = float(env.param_bounds[1, 1])
    log_I0_low = np.log(I0_LOW)
    log_I0_high = np.log(I0_HIGH)

    def _x_to_params(x):
        """ä¼˜åŒ–å˜é‡ x = [I_ph, log_I0, n, Rs, Rsh] -> ç‰©ç†å‚æ•° [I_ph, I0, n, Rs, Rsh]"""
        return np.array([x[0], np.exp(x[1]), x[2], x[3], x[4]], dtype=np.float64)

    def _scipy_objective(x):
        """ä¾› scipy è°ƒç”¨çš„ç›®æ ‡å‡½æ•°ã€‚x ä¸­ I0 ä¸º log(I0)ã€‚"""
        try:
            params = _x_to_params(x)
            return env._objective_function(params, env.V, env.I)
        except Exception:
            return 1e10

    bounds_linear = [
        (float(env.param_bounds[i, 0]), float(env.param_bounds[i, 1]))
        for i in range(5)
    ]
    bounds_log_I0 = [
        (bounds_linear[0][0], bounds_linear[0][1]),   # I_ph
        (log_I0_low, log_I0_high),                       # log(I0)
        (bounds_linear[2][0], bounds_linear[2][1]),   # n
        (bounds_linear[3][0], bounds_linear[3][1]),   # Rs
        (bounds_linear[4][0], bounds_linear[4][1]),   # Rsh
    ]
    # ä½¿ç”¨æ•°æ®é©±åŠ¨çš„åˆå€¼ï¼šI_ph â‰ˆ I_scï¼ˆçŸ­è·¯ç”µæµï¼‰
    I_sc_data = float(np.max(env.I))
    x0_linear = env.default_params.copy()
    x0_linear[0] = np.clip(I_sc_data * 1.02, env.param_bounds[0, 0], env.param_bounds[0, 1])
    x0 = np.array([
        x0_linear[0],
        np.log(max(x0_linear[1], 1e-20)),
        x0_linear[2], x0_linear[3], x0_linear[4]
    ])
    initial_error = _scipy_objective(x0)
    print(f"[ä¼ ç»Ÿä¼˜åŒ–] åˆå§‹è¯¯å·® (default_params): {initial_error:.6f}")

    # å¤šç»„éšæœºç§å­è·‘ DEï¼Œå–æœ€ä¼˜ï¼›åŠ å¼ºæœç´¢ä»¥äº‰å–è¯¯å·® ~0.3
    best_de_x = x0.copy()
    best_de_fun = float("inf")
    de_seeds = [42, 123, 456, 789, 2024]  # 5 ä¸ªç§å­ï¼Œæé«˜æ‰¾åˆ°æ›´ä¼˜è§£æ¦‚ç‡
    for run, seed in enumerate(de_seeds):
        print(f"[ä¼ ç»Ÿä¼˜åŒ–] å…¨å±€ä¼˜åŒ– ç¬¬ {run+1}/{len(de_seeds)} æ¬¡ (seed={seed})ï¼Œè¯·ç¨å€™...")
        result_de = differential_evolution(
            _scipy_objective,
            bounds_log_I0,
            strategy="best1bin",
            maxiter=1000,
            popsize=50,
            tol=1e-6,
            seed=seed,
            polish=True,
            disp=False,
            atol=1e-8,
        )
        if result_de.fun < best_de_fun:
            best_de_fun = result_de.fun
            best_de_x = result_de.x
    result_de.x = best_de_x
    result_de.fun = best_de_fun
    best_params_de = _x_to_params(best_de_x)
    error_de = best_de_fun
    if error_de < initial_error:
        env.default_params = best_params_de
        print(f"[ä¼ ç»Ÿä¼˜åŒ–] å…¨å±€ä¼˜åŒ–æœ€ä½³è¯¯å·®: {error_de:.6f}ï¼Œå°†ä½œä¸º RL çš„é»˜è®¤å‚æ•°")
    else:
        print(f"[ä¼ ç»Ÿä¼˜åŒ–] å…¨å±€ä¼˜åŒ–æœªä¼˜äºåˆå€¼ (error={error_de:.6f})ï¼Œä¿ç•™åŸ default_params")

    # åœ¨å…¨å±€ç»“æœåŸºç¡€ä¸Šç”¨ L-BFGS-B ç²¾ä¿®ï¼ˆä¸¤è½®ç²¾ä¿®ï¼Œç¬¬äºŒè½®ä»ç¬¬ä¸€è½®ç»“æœå‡ºå‘ï¼‰
    try:
        x_start = result_de.x.copy()
        best_so_far = float(result_de.fun)
        for pass_idx in range(2):
            result_lbfgs = minimize(
                _scipy_objective, x_start,
                method="L-BFGS-B", bounds=bounds_log_I0,
                options={"maxiter": 1000, "ftol": 1e-12},
            )
            if result_lbfgs.success and result_lbfgs.fun < best_so_far:
                env.default_params = _x_to_params(result_lbfgs.x)
                x_start = result_lbfgs.x.copy()
                best_so_far = result_lbfgs.fun
                print(f"[ä¼ ç»Ÿä¼˜åŒ–] L-BFGS-B ç¬¬{pass_idx+1}è½®ç²¾ä¿®åè¯¯å·®: {result_lbfgs.fun:.6f}")
            else:
                break
    except Exception:
        pass

    # æ˜ç¡®æ‰“å° RL èµ·ç‚¹è¯¯å·®ï¼Œä¾¿äºæ’æŸ¥â€œè¯¯å·®å¡åœ¨ 0.34â€æ˜¯å¦å› ä¼ ç»Ÿä¼˜åŒ–åªåšåˆ°è¯¥æ°´å¹³
    rl_start_error = env._objective_function(env.default_params, env.V, env.I)
    print(f"[ä¼ ç»Ÿä¼˜åŒ–] RL å°†ä»æ­¤è¯¯å·®èµ·ç‚¹å¼€å§‹: {rl_start_error:.6f}")

    # è¯Šæ–­ï¼šè¯¯å·®ä¸»è¦æ¥è‡ªå“ªäº›ç‚¹ï¼ˆä¾¿äºåˆ¤æ–­æ˜¯å¦è¢«å°‘æ•°ç‚¹â€œç»‘æ¶â€ï¼‰
    try:
        I_sim = env._solar_cell_model(env.V, env.default_params)
        valid = env.I > 1e-10
        rel_err = np.zeros_like(env.I)
        rel_err[valid] = np.abs((env.I[valid] - I_sim[valid]) / env.I[valid])
        idx = np.argsort(rel_err)[::-1][:5]
        print("[ä¼ ç»Ÿä¼˜åŒ–] ç›¸å¯¹è¯¯å·®æœ€å¤§çš„ 5 ä¸ªç‚¹: V, I_meas, rel_err =")
        for i in idx:
            if valid[i]:
                print(f"  {env.V[i]:.4f}, {env.I[i]:.6e}, {rel_err[i]:.4f}")
    except Exception:
        pass

    agent = TD3Agent(
        state_dim=STATE_DIM,
        action_dim=ACTION_DIM,
        hidden_dim=HIDDEN_DIM,
        lr_actor=LR_ACTOR,
        lr_critic=LR_CRITIC,
        gamma=GAMMA,
        tau=TAU,
        buffer_capacity=BUFFER_CAPACITY,
        batch_size=BATCH_SIZE,
        policy_noise=0.1,   # é€‚å½“å¢å¤§ç›®æ ‡ç­–ç•¥å¹³æ»‘å™ªå£°ï¼Œåˆ©äºæ¢ç´¢
        noise_clip=0.15,   # å™ªå£°è£å‰ªèŒƒå›´ç•¥æ”¾å®½
        policy_delay=2      # æ¯ 2 æ¬¡ Critic æ›´æ–° 1 æ¬¡ Actor
    )
    # æ›¿æ¢é»˜è®¤å™ªå£°ï¼ˆä¹Ÿå¯åœ¨Agentåˆå§‹åŒ–æ—¶ç›´æ¥ä¼ å…¥å‚æ•°ï¼‰
    agent.noise = GaussianNoise(ACTION_DIM, std=NOISE_STD, std_decay=NOISE_DECAY, std_min=NOISE_MIN)

    # ========== è®­ç»ƒè®°å½• ==========
    best_error_overall = float('inf')
    best_params_overall = None  # å†å²æœ€ä½³å‚æ•°ï¼Œç”¨äºçƒ­å¯åŠ¨ï¼Œé¿å…æ¯è½®éƒ½ä» 0.34 é‡æ¥
    episode_rewards = []
    episode_errors = []

    # ========== è®­ç»ƒå¾ªç¯ ==========
    for episode in range(1, NUM_EPISODES + 1):
        # ä¼˜å…ˆä»å†å²æœ€ä½³å‚æ•°çƒ­å¯åŠ¨ï¼ˆ>1 è½®ä¸”å·²æœ‰æœ€ä½³ï¼‰ï¼Œçº¦ 30% ä»ä¼ ç»Ÿä¼˜åŒ–èµ·ç‚¹+æ‰°åŠ¨æ¢ç´¢
        use_perturb = (episode > 3) and (episode % 3 == 0)
        if episode > 1 and best_params_overall is not None and not use_perturb:
            start_params = best_params_overall
        elif episode > 1 and best_params_overall is not None and use_perturb:
            start_params = best_params_overall  # åœ¨æœ€ä½³ç‚¹é™„è¿‘æ‰°åŠ¨
        else:
            start_params = None  # ç¬¬ 1 è½®æˆ–æ˜¾å¼ç”¨ default_params
        state = env.reset(perturb=use_perturb, initial_params=start_params)
        episode_reward = 0
        agent.noise.reset()  # æ¯ä¸ªepisodeé‡ç½®å™ªå£°ï¼Œè®©æ¢ç´¢å¼ºåº¦é‡æ–°å¼€å§‹

        for step in range(MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œï¼ˆæ·»åŠ å™ªå£°ï¼‰
            action = agent.select_action(state, add_noise=True)
            # ç¯å¢ƒäº¤äº’
            next_state, reward, done, info = env.step(action)
            # å­˜å‚¨ç»éªŒ
            agent.replay_buffer.push(state, action, reward, next_state, done)
            # æ›´æ–°ç½‘ç»œ
            agent.update()
            # è½¬ç§»çŠ¶æ€
            state = next_state
            episode_reward += reward
            if done:
                break

        # è®°å½•æœ¬episodeçš„ç»Ÿè®¡
        episode_rewards.append(episode_reward)
        episode_errors.append(env.best_error)
        if env.best_error < best_error_overall:
            best_error_overall = env.best_error
            best_params_overall = env.best_params.copy()  # ä¸‹ä¸€è½®ä»æ­¤çƒ­å¯åŠ¨
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'actor': agent.actor.state_dict(),
                'critic1': agent.critic1.state_dict(),
                'critic2': agent.critic2.state_dict(),
                'best_error': best_error_overall,
            }, os.path.join(SAVE_DIR, 'best_model.pth'))
        elif best_params_overall is None:
            # é¦–è½®æœªæ”¹è¿›æ—¶ä¹Ÿè®°å½•å½“å‰æœ€ä½³ï¼Œä¾¿äºç¬¬ 2 è½®çƒ­å¯åŠ¨
            best_params_overall = env.best_params.copy()
            best_error_overall = env.best_error

        # æ‰“å°è¿›åº¦ï¼ˆå¸¦çƒ­å¯åŠ¨æç¤ºï¼‰
        if episode % 5 == 0:
            warm = " [ä»å†å²æœ€ä½³çƒ­å¯åŠ¨]" if (episode > 1 and start_params is not None) else ""
            print(f"Episode {episode:3d} | Reward: {episode_reward:8.2f} | Best Error: {env.best_error:.6f} | å…¨å±€æœ€ä½³: {best_error_overall:.6f}{warm}")

        # æå‰åœæ­¢
        if best_error_overall < TARGET_ERROR:
            print(f"ğŸ¯ Target error reached at episode {episode}. Stopping training.")
            break

    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³è¯¯å·®: {best_error_overall}")

    # å¯é€‰ï¼šç»˜åˆ¶å¥–åŠ±å’Œè¯¯å·®æ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(episode_rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Episode Rewards")
        plt.subplot(1, 2, 2)
        plt.plot(episode_errors)
        plt.xlabel("Episode")
        plt.ylabel("Best Error")
        plt.title("Best Error per Episode")
        plt.tight_layout()
        plt.savefig(os.path.join(SAVE_DIR, "training_curve.png"))
        plt.show()
    except ImportError:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")


if __name__ == "__main__":
    main()